{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_0-LXx2gSw8",
    "outputId": "1108a315-411d-4a9b-f4cd-8bb090d1c637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEa3F0xHfUvj",
    "outputId": "d6fc18de-2463-4750-eb54-9a6d795614e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Sample: i was recently told that in china their had been strong protests against the release of red corner \u0016 and this is apparently because of the way it shows the injustice of many chinese laws . but if you ask me , the real truth of the matter is that the chinese critics association were determined not to punish the population into viewing richard gere running across rooftops in search for a fellow american . or more the point , anyone that allows him to bask in his own less - than - subtle presence . this is not an...\n",
      "Label: neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "# NLTK의 movie_reviews 데이터셋 다운로드\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# 영화 리뷰와 해당 라벨을 데이터셋으로 로드\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# 데이터를 셔플링하여 무작위로 섞음\n",
    "random.shuffle(documents)\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "sample_review, sample_label = documents[0]\n",
    "print(f\"Review Sample: {' '.join(sample_review[:100])}...\")\n",
    "print(f\"Label: {sample_label}\")\n",
    "\n",
    "# 불용어\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okJdj0uOfenn",
    "outputId": "2e7d846a-730b-4264-90bc-c57d95f3e9a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuracy:  0.8300\n"
     ]
    }
   ],
   "source": [
    "# 리뷰 텍스트와 라벨 분리하고, 불용어 제거\n",
    "reviews = [' '.join([word for word in review if word.lower() not in stop_words])  for review, category in documents]\n",
    "\n",
    "## 라벨을 분리\n",
    "label = [category for review, category in documents]\n",
    "\n",
    "# 리뷰데이터 임베딩 변환 (학습에 필요한 리뷰와 라벨을 분리하여 진행)\n",
    "Vactorizer = CountVectorizer()\n",
    "X = Vactorizer.fit_transform(reviews)\n",
    "y = label\n",
    "\n",
    "# 데이터셋 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 111)\n",
    "\n",
    "# Navie Bayes 분류기 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Base Model Accuracy: {acc: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZVFa0sKi3zZ"
   },
   "source": [
    "### 전처리를 어떤 식으로 하면 좋을까?\n",
    "- 단어 정규화를 통해 성능 향상\n",
    "- 어간, 표제어 추출하는 방법\n",
    "- countvectorizer 빈도에 대한 기준을 다르게 하는 방법\n",
    " -> 빈도가 너무 낮은 값들은 제거하고, 어느 정도 빈도를 기준을 추가\n",
    "- TF-IDF로 진행해서 새롭게 벡터를 만드는 방법\n",
    "- N-gram으로 추가하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EeBMzdHi4B_",
    "outputId": "0d362044-0e69-4ba8-a3bf-47fc79236470"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# NLTK의 movie_reviews 데이터셋 다운로드\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 영화 리뷰와 해당 라벨을 데이터셋으로 로드\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# 데이터를 셔플링하여 무작위로 섞음\n",
    "random.shuffle(documents)\n",
    "\n",
    "# 불용어\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kgiyMXaC3oF"
   },
   "outputs": [],
   "source": [
    "# 1. 문장 및 단어 정규화\n",
    "def normalize_text(text):\n",
    "  text = text.lower() # 소문자\n",
    "  text = re.sub(r'\\d+', '', text) # 숫자제거\n",
    "  text = re.sub(r'[^\\w\\s]', '', text) # 특수문자 제거\n",
    "  return text\n",
    "\n",
    "#전처리 진행\n",
    "normalized_reviews = [normalize_text(' '.join(review)) for review, category in documents]\n",
    "\n",
    "# 2. 어간추출 (stemming)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_reviews = [' '.join([stemmer.stem(word) for word in review.split()]) for review in normalized_reviews]\n",
    "\n",
    "# 3. 표제어추출\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_reviews = [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in stemmed_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s5xtyCjEQmO"
   },
   "source": [
    "### 어간추출로 진행하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCuXI6GwEQH_",
    "outputId": "374b03f3-ce87-4bc8-9a01-3fe875c9d002"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stem & CountVectorizer': 0.8175,\n",
       " 'stem & min_df': 0.815,\n",
       " 'stem & bigram': 0.8475,\n",
       " 'stem & tfidf': 0.8175}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 4. CounterVectorizer 하이퍼파라미터 조정\n",
    "# 최소 빈도수 설정해서 희귀 단어 제거 가능\n",
    "\n",
    "# 2번의 어간추출, 표제어추출로 학습 테스트\n",
    "vectorizer_min_df = CountVectorizer(min_df = 2)\n",
    "X_min_df = vectorizer_min_df.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# N-gram 사용 (2,2)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range = (2,2))\n",
    "X_bigram = vectorizer_bigram.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(lemmatized_reviews)\n",
    "\n",
    "## 데이터 분할\n",
    "\n",
    "# 1. stem\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split(stemmed_reviews, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 2. min_df\n",
    "X_train_min_df, X_test_min_df, y_train_min_df, y_test_min_df = train_test_split(X_min_df, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 3. bigram\n",
    "X_train_bigram, X_test_bigram, y_train_bigram, y_test_bigram = train_test_split(X_bigram, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 4. TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "## Naive Bayes 분류기 사용하여 학습, 평가\n",
    "\n",
    "# 1. Navie Bayes 분류기 학습\n",
    "model_stem = MultinomialNB()\n",
    "model_stem.fit(vectorizer.fit_transform(X_train_stem), y_train_stem)\n",
    "y_pred_stem = model_stem.predict(vectorizer.transform(X_test_stem))\n",
    "accuracy_stem = accuracy_score(y_test_stem, y_pred_stem)\n",
    "\n",
    "# 2. min_df\n",
    "model_min_df = MultinomialNB()\n",
    "model_min_df.fit(X_train_min_df, y_train_min_df)\n",
    "y_pred_min_df = model_min_df.predict(X_test_min_df)\n",
    "accuracy_min_df = accuracy_score(y_test_min_df, y_pred_min_df)\n",
    "\n",
    "# 3. bigram\n",
    "model_bigram = MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, y_train_bigram)\n",
    "y_pred_bigram = model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(y_test_bigram, y_pred_bigram)\n",
    "\n",
    "# 4. TF-IDF\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n",
    "\n",
    "{\n",
    "    'stem & CountVectorizer': accuracy_stem,\n",
    "    'stem & min_df': accuracy_min_df,\n",
    "    'stem & bigram': accuracy_bigram,\n",
    "    'stem & tfidf': accuracy_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8banyRrYHKn6"
   },
   "source": [
    "### 표제어추출로 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qF5ggYg_HMVf",
    "outputId": "478eeb5d-21e0-422a-b611-6a24a1daf038"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma & CountVectorizer': 0.815,\n",
       " 'lemma & min_df': 0.815,\n",
       " 'lemma & bigram': 0.8475,\n",
       " 'lemma & tfidf': 0.8175}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 4. CounterVectorizer 하이퍼파라미터 조정\n",
    "# 최소 빈도수 설정해서 희귀 단어 제거 가능\n",
    "\n",
    "# 2번의 어간추출, 표제어추출로 학습 테스트\n",
    "vectorizer_min_df = CountVectorizer(min_df = 2)\n",
    "X_min_df = vectorizer_min_df.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# N-gram 사용 (2,2)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range = (2,2))\n",
    "X_bigram = vectorizer_bigram.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(lemmatized_reviews)\n",
    "\n",
    "## 데이터 분할\n",
    "\n",
    "# 1. stem\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split(lemmatized_reviews, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 2. min_df\n",
    "X_train_min_df, X_test_min_df, y_train_min_df, y_test_min_df = train_test_split(X_min_df, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 3. bigram\n",
    "X_train_bigram, X_test_bigram, y_train_bigram, y_test_bigram = train_test_split(X_bigram, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 4. TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "## Naive Bayes 분류기 사용하여 학습, 평가\n",
    "\n",
    "# 1. stem\n",
    "model_stem = MultinomialNB()\n",
    "model_stem.fit(vectorizer.fit_transform(X_train_stem), y_train_stem)\n",
    "y_pred_stem = model_stem.predict(vectorizer.transform(X_test_stem))\n",
    "accuracy_stem = accuracy_score(y_test_stem, y_pred_stem)\n",
    "\n",
    "# 2. min_df\n",
    "model_min_df = MultinomialNB()\n",
    "model_min_df.fit(X_train_min_df, y_train_min_df)\n",
    "y_pred_min_df = model_min_df.predict(X_test_min_df)\n",
    "accuracy_min_df = accuracy_score(y_test_min_df, y_pred_min_df)\n",
    "\n",
    "# 3. bigram\n",
    "model_bigram = MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, y_train_bigram)\n",
    "y_pred_bigram = model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(y_test_bigram, y_pred_bigram)\n",
    "\n",
    "# 4. TF-IDF\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n",
    "\n",
    "{\n",
    "    'lemma & CountVectorizer': accuracy_stem,\n",
    "    'lemma & min_df': accuracy_min_df,\n",
    "    'lemma & bigram': accuracy_bigram,\n",
    "    'lemma & tfidf': accuracy_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aC_OubVkH_A1",
    "outputId": "be8ef9d5-1635-465f-bed3-c77ae43ea276"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1600x444832 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 918516 stored elements in Compressed Sparse Row format>,\n",
       " <1600x16033 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 496089 stored elements in Compressed Sparse Row format>,\n",
       " <1600x25578 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 503746 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bigram, X_train_min_df, X_train_tfidf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45ae3de9",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1723006699314,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "45ae3de9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c29f20f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25270,
     "status": "ok",
     "timestamp": 1723006724580,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "c29f20f0",
    "outputId": "0f57e8a4-9f87-4b01-904e-3b9de412a34a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Sample: \" gattaca \" represents a solid breakthrough in the recent onslaught of science - fiction films -- it ' s a genre picture that doesn ' t rely on alien creatures or loud explosions to tell its story . the movie takes place in a futuristic world where babies are created through genetic tampering and not sexual reproduction . this allows parents to predetermine what kind of eye color , intelligence and life span they ' d like for their child , and also eliminates most pesky chances of health defects . those made the old - fashioned way are...\n",
      "Label: pos\n"
     ]
    }
   ],
   "source": [
    "# 영화 리뷰와 해당 라벨을 데이터셋으로 로드\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# 데이터를 셔플링하여 무작위로 섞음\n",
    "random.shuffle(documents)\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "sample_review, sample_label = documents[0]\n",
    "print(f\"Review Sample: {' '.join(sample_review[:100])}...\")\n",
    "print(f\"Label: {sample_label}\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9690bc4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1eXwHp4gChdT3hXThP_M05Vw4k_mt9wiR"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1723006724580,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "9690bc4c",
    "outputId": "daec8cc0-edc5-4e86-e4ad-6d90f0485956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 리뷰 텍스트와 라벨 분리하고, 불용어 제거\n",
    "reviews = [' '.join([word for word in review if word.lower() not in stop_words]) for review, category in documents]\n",
    "\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2eefb79b",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1723006724580,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "2eefb79b"
   },
   "outputs": [],
   "source": [
    "# 라벨을 분리\n",
    "label = [category for review, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91107ce4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1723006725604,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "91107ce4",
    "outputId": "f50bc0db-67fb-4e73-9cbd-b9066fa6bc1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuarcy : 0.8100\n"
     ]
    }
   ],
   "source": [
    "# 학습에 필요한 리뷰와 라벨을 분리하여 진행\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "y = label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=111)\n",
    "\n",
    "# NaiveBayes 분류기 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 모델 정확도를 예측\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Base Model Accuarcy : {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07adaf",
   "metadata": {
    "id": "7c07adaf"
   },
   "source": [
    "### 전처리를 어떤 식으로 하면 좋을까?\n",
    "- 단어 정규화를 통해 성능 향상\n",
    "- 어간, 표제어 추출하는 방법\n",
    "- countvectorizer 빈도에 대한 기준을 다르게 하는 방법\n",
    "    - 빈도가 너무 낮은 값들은 제거하고, 어느 정도 빈도를 기준을 추가\n",
    "- TF-IDF로 진행해서 새롭게 벡터를 만드는 방법\n",
    "- N-gram으로 추가하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8f3468a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1723006725604,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "a8f3468a",
    "outputId": "8eabbf8d-d472-4017-a762-3637de35593f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7aeecefb",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1723006725604,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "7aeecefb"
   },
   "outputs": [],
   "source": [
    "# 1. 문장 및 단어 정규화\n",
    "def normalize_text(text):\n",
    "    text = text.lower() # 소문자\n",
    "    text = re.sub(r'\\d+','',text) # 숫자 제거\n",
    "    text = re.sub(r'[^\\w\\s]','',text) # 구두점 제거\n",
    "    return text\n",
    "\n",
    "# 전처리 진행\n",
    "normalized_reviews = [normalize_text(' '.join(review)) for review, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e670721e",
   "metadata": {
    "executionInfo": {
     "elapsed": 51852,
     "status": "ok",
     "timestamp": 1723006777453,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "e670721e"
   },
   "outputs": [],
   "source": [
    "# 2.어간 추출\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_reviews = [' '.join([stemmer.stem(word) for word in review.split()]) for review in normalized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7bb99a4f",
   "metadata": {
    "executionInfo": {
     "elapsed": 5390,
     "status": "ok",
     "timestamp": 1723006782840,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "7bb99a4f"
   },
   "outputs": [],
   "source": [
    "# 3. 표제어 추출\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_reviews = [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in normalized_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2637117",
   "metadata": {
    "id": "b2637117"
   },
   "source": [
    "- 정규식, nltk 패키지로 리뷰 데이터 전처리\n",
    "- 임베딩을 통한 행렬 변환\n",
    "- n_gram으로 추가해서 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fc9ff",
   "metadata": {
    "id": "7d9fc9ff"
   },
   "source": [
    "### 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5bbe175",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1723006782840,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "c5bbe175"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f47a2f1",
   "metadata": {
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1723006783885,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "5f47a2f1"
   },
   "outputs": [],
   "source": [
    "# 4. countvectorizer 하이퍼 파라미터 조정\n",
    "# 최소빈도수 설정해서 희귀 단어 제거 가능\n",
    "vectorizer_min_df = CountVectorizer(min_df=2)\n",
    "\n",
    "# 2번의 어간 추출, 표제어 추출로 학습 테스트\n",
    "X_min_df = vectorizer_min_df.fit_transform(stemmed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97e953e0",
   "metadata": {
    "executionInfo": {
     "elapsed": 4651,
     "status": "ok",
     "timestamp": 1723006788532,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "97e953e0"
   },
   "outputs": [],
   "source": [
    "# 5. N-그램 사용 -> (2, 2)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2))\n",
    "X_bigram = vectorizer_bigram.fit_transform(stemmed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bd23a91",
   "metadata": {
    "executionInfo": {
     "elapsed": 1189,
     "status": "ok",
     "timestamp": 1723006789704,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "7bd23a91"
   },
   "outputs": [],
   "source": [
    "# 6. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(stemmed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5878130",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1017,
     "status": "ok",
     "timestamp": 1723006790718,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "b5878130",
    "outputId": "7edfdc2d-0d17-4e67-a1f4-3abf0375daf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stem & Countvectorizer': 0.825,\n",
       " 'stem & min_df': 0.8275,\n",
       " 'stem & bigram': 0.8625,\n",
       " 'stem & tfidf': 0.835}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "\n",
    "# 1. stem\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split(stemmed_reviews, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "# 2. X_min_df\n",
    "X_train_min_df, X_test_min_df, y_train_min_df, y_test_min_df = train_test_split(X_min_df, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "# 3. X_bigram\n",
    "X_train_bigram, X_test_bigram, y_train_bigram, y_test_bigram = train_test_split(X_bigram, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "# 4. TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "\n",
    "# Naive Bayes 분류기 사용하여 학습, 평가\n",
    "\n",
    "# 1. Navie Bayes 분류기 학습\n",
    "model_stem = MultinomialNB()\n",
    "model_stem.fit(vectorizer.fit_transform(X_train_stem), y_train_stem)\n",
    "y_pred_stem = model_stem.predict(vectorizer.transform(X_test_stem))\n",
    "accuracy_stem = accuracy_score(y_test_stem, y_pred_stem)\n",
    "\n",
    "# 2. min_df Navie Bayes 분류기 학습\n",
    "model_min_df = MultinomialNB()\n",
    "model_min_df.fit(X_train_min_df, y_train_min_df)\n",
    "y_pred_min_df = model_min_df.predict(X_test_min_df)\n",
    "accuracy_min_df = accuracy_score(y_test_min_df, y_pred_min_df)\n",
    "\n",
    "# 3. X_bigram Navie Bayes 분류기 학습\n",
    "model_bigram = MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, y_train_bigram)\n",
    "y_pred_bigram = model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(y_test_bigram, y_pred_bigram)\n",
    "\n",
    "# 4. TF-IDF Navie Bayes 분류기 학습\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n",
    "\n",
    "{\n",
    "    'stem & Countvectorizer' : accuracy_stem,\n",
    "    'stem & min_df' : accuracy_min_df,\n",
    "    'stem & bigram': accuracy_bigram,\n",
    "    'stem & tfidf' : accuracy_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aOUgR4ZOE_0K",
   "metadata": {
    "id": "aOUgR4ZOE_0K"
   },
   "source": [
    "### 필수과제1\n",
    "- base로만 진행했지만, 추가적으로 임베딩을 다양하게 진행해 보시면서 0.845 의 성능보다 더 올리기\n",
    "- 다양한 텍스트 전처리를 통해 성능 개선하기\n",
    "- 파생변수를 추가해도 괜찮음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f6caa",
   "metadata": {
    "id": "db6f6caa"
   },
   "source": [
    "### 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d921da0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10664,
     "status": "ok",
     "timestamp": 1723006996449,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "8d921da0",
    "outputId": "da238bfc-be6e-4c2c-9739-526b108b0fc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma & Countvectorizer': 0.82,\n",
       " 'lemma & min_df': 0.8325,\n",
       " 'lemma & bigram': 0.835,\n",
       " 'lemma & tfidf': 0.845}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 4. countvectorizer 하이퍼파라미터 조정\n",
    "# 최소빈도수 설정해서 희귀 단어 제거 가능\n",
    "vectorizer_min_df = CountVectorizer(min_df=12)\n",
    "\n",
    "# 2번의 어간추출, 표제어 추출로 학습 테스트\n",
    "X_min_df = vectorizer_min_df.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# N-그램 사용 -> (2, 2)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), min_df=12)\n",
    "X_bigram = vectorizer_bigram.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=12)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(lemmatized_reviews)\n",
    "\n",
    "\n",
    "# 데이터 분할\n",
    "\n",
    "# 1. stem\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split(lemmatized_reviews, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "# 2. X_min_df\n",
    "X_train_min_df, X_test_min_df, y_train_min_df, y_test_min_df = train_test_split(X_min_df, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "# 3. X_bigram\n",
    "X_train_bigram, X_test_bigram, y_train_bigram, y_test_bigram = train_test_split(X_bigram, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "# 4. TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "\n",
    "# Naive Bayes 분류기 사용하여 학습, 평가\n",
    "\n",
    "# 1. Navie Bayes 분류기 학습\n",
    "model_stem = MultinomialNB()\n",
    "model_stem.fit(vectorizer.fit_transform(X_train_stem), y_train_stem)\n",
    "y_pred_stem = model_stem.predict(vectorizer.transform(X_test_stem))\n",
    "accuracy_stem = accuracy_score(y_test_stem, y_pred_stem)\n",
    "\n",
    "# 2. min_df Navie Bayes 분류기 학습\n",
    "model_min_df = MultinomialNB()\n",
    "model_min_df.fit(X_train_min_df, y_train_min_df)\n",
    "y_pred_min_df = model_min_df.predict(X_test_min_df)\n",
    "accuracy_min_df = accuracy_score(y_test_min_df, y_pred_min_df)\n",
    "\n",
    "# 3. X_bigram Navie Bayes 분류기 학습\n",
    "model_bigram = MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, y_train_bigram)\n",
    "y_pred_bigram = model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(y_test_bigram, y_pred_bigram)\n",
    "\n",
    "# 4. TF-IDF Navie Bayes 분류기 학습\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n",
    "\n",
    "{\n",
    "    'lemma & Countvectorizer' : accuracy_stem,\n",
    "    'lemma & min_df' : accuracy_min_df,\n",
    "    'lemma & bigram' : accuracy_bigram,\n",
    "    'lemma & tfidf' : accuracy_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "gEAxjwfmFAa3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4940,
     "status": "ok",
     "timestamp": 1723006820584,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "gEAxjwfmFAa3",
    "outputId": "3bd5e7bb-6234-47f3-8238-691aeb0ba59e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trigram': 0.815}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_trigram = CountVectorizer(ngram_range=(3, 3), min_df=2)\n",
    "X_trigram = vectorizer_trigram.fit_transform(lemmatized_reviews)\n",
    "X_train_trigram, X_test_trigram, y_train_trigram, y_test_trigram = train_test_split(X_trigram, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "model_trigram = MultinomialNB()\n",
    "model_trigram.fit(X_train_trigram, y_train_trigram)\n",
    "y_pred_trigram = model_trigram.predict(X_test_trigram)\n",
    "accuracy_trigram = accuracy_score(y_test_trigram, y_pred_trigram)\n",
    "\n",
    "{\n",
    "    'Trigram': accuracy_trigram\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reaBm0UNKh7s",
   "metadata": {
    "id": "reaBm0UNKh7s"
   },
   "source": [
    "- Trigram을 사용했을 때 오히려 성능이 더 안좋아졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "T4eLpY9pG-vI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18503,
     "status": "ok",
     "timestamp": 1723006846133,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "T4eLpY9pG-vI",
    "outputId": "26d3d7ea-e42e-41d0-a52e-829db332e272"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "preprocessed_reviews = [preprocess_text(' '.join(review)) for review, _ in documents]\n",
    "\n",
    "labels = [label for _, label in documents]\n",
    "\n",
    "# Word2Vec 임베딩 방식을 적용\n",
    "word2vec_model = Word2Vec(preprocessed_reviews, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "def document_vector(words):\n",
    "    words = [word for word in words if word in word2vec_model.wv.key_to_index]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(word2vec_model.wv[words], axis=0)\n",
    "\n",
    "document_vectors = np.array([document_vector(review) for review in preprocessed_reviews])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(document_vectors, labels, test_size=0.2, random_state=111)\n",
    "\n",
    "model_w2v = GaussianNB()\n",
    "model_w2v.fit(X_train, y_train)\n",
    "y_pred_w2v = model_w2v.predict(X_test)\n",
    "accuracy_w2v = accuracy_score(y_test, y_pred_w2v)\n",
    "\n",
    "accuracy_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "y8hck3t_H3_s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70581,
     "status": "ok",
     "timestamp": 1723006916712,
     "user": {
      "displayName": "이정현",
      "userId": "12065954518616664698"
     },
     "user_tz": -540
    },
    "id": "y8hck3t_H3_s",
    "outputId": "8b4ad9bb-71ce-41a0-df5f-867a3ef1b921"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "preprocessed_reviews = [preprocess_text(' '.join(review)) for review, _ in documents]\n",
    "\n",
    "labels = [label for _, label in documents]\n",
    "\n",
    "# FastText 임베딩을 사용\n",
    "fasttext_model = FastText(preprocessed_reviews, vector_size=100, window=5, min_count=2, workers=4, sg=1)\n",
    "\n",
    "def document_vector(words):\n",
    "    words = [word for word in words if word in fasttext_model.wv.key_to_index]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(fasttext_model.wv[words], axis=0)\n",
    "\n",
    "fasttext_vectors = np.array([document_vector(review) for review in preprocessed_reviews])\n",
    "\n",
    "raw_reviews = [' '.join(review) for review in preprocessed_reviews]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=12)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(raw_reviews)\n",
    "\n",
    "combined_features = hstack([tfidf_vectors, np.array(fasttext_vectors)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.2, random_state=111)\n",
    "\n",
    "model_fasttext = GaussianNB()\n",
    "model_fasttext.fit(X_train.toarray(), y_train)\n",
    "y_pred_fasttext = model_fasttext.predict(X_test.toarray())\n",
    "accuracy_fasttext = accuracy_score(y_test, y_pred_fasttext)\n",
    "\n",
    "accuracy_fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X205TL5BL83H",
   "metadata": {
    "id": "X205TL5BL83H"
   },
   "source": [
    "- 성능을 더 개선해보려 하였지만 실패하였고 bigram + stem(어간추출) 방식이 0.8625로 가장 좋았다.\n",
    "- min_df를 조정하였을 때 성능이 약간 상승하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vDKtQHxAMGQw",
   "metadata": {
    "id": "vDKtQHxAMGQw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

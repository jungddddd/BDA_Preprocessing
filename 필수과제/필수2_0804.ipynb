{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGSRqRsRIxlO"
   },
   "source": [
    "### 필수과제1\n",
    "- base로만 진행했지만, 추가적으로 임베딩을 다양하게 진행해 보시면서 0.845 의 성능보다 더 올리기\n",
    "- 다양한 텍스트 전처리를 통해 성능 개선하기\n",
    "- 파생변수를 추가해도 괜찮음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnEJCgHRWbU0",
    "outputId": "13683001-b3f1-4309-ddb1-af0af5e37b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OwkZRJ1IsdR",
    "outputId": "951ecdb2-9331-49c4-9ed3-cb126d8318e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Sample: hollywood never fails to astound me . every time i think those coked - up little buggers have hit rock bottom ; they come up with a new excavating tool . i was truly convinced that \" wild wild west \" marked 1999 studio filmmaking at its most hapless , but then along comes \" end of days \" to prove me wrong . this big budget , brain dead apocalyptic thriller bludgeons its audience with overwrought music , grisly violence , explosions galore and lots of cheesy special effects . the result ? nothing but groans and yawns ....\n",
      "Label: neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# NLTK의 movie_reviews 데이터셋 다운로드\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# 영화 리뷰와 해당 라벨을 데이터셋으로 로드\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# 데이터를 셔플링하여 무작위로 섞음\n",
    "random.shuffle(documents)\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "sample_review, sample_label = documents[0]\n",
    "print(f\"Review Sample: {' '.join(sample_review[:100])}...\")\n",
    "print(f\"Label: {sample_label}\")\n",
    "\n",
    "# 불용어\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8ucmuk7KEjK",
    "outputId": "cc837c75-c105-4da8-e253-eb1e36f67dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuracy:  0.8275\n"
     ]
    }
   ],
   "source": [
    "# 리뷰 텍스트와 라벨 분리하고, 불용어 제거\n",
    "reviews = [' '.join([word for word in review if word.lower() not in stop_words])  for review, category in documents]\n",
    "label = [category for review, category in documents]\n",
    "\n",
    "# 리뷰데이터 임베딩 변환\n",
    "Vactorizer = CountVectorizer()\n",
    "X = Vactorizer.fit_transform(reviews)\n",
    "y = label\n",
    "\n",
    "# 데이터셋 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 111)\n",
    "\n",
    "# Navie Bayes 분류기 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측값\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Base Model Accuracy: {acc: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZxpegXpW5_A",
    "outputId": "aa2aaa30-582a-4d41-ba2c-8311af21eb26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK의 movie_reviews 데이터셋 다운로드\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 영화 리뷰와 해당 라벨을 데이터셋으로 로드\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# 데이터를 셔플링하여 무작위로 섞음\n",
    "random.shuffle(documents)\n",
    "\n",
    "# 불용어\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GT7mR5rPWBWi"
   },
   "outputs": [],
   "source": [
    "# 1. 문장 및 단어 정규화\n",
    "def normalize_text(text):\n",
    "  text = text.lower() # 소문자\n",
    "  text = re.sub(r'\\d+', '', text) # 숫자제거\n",
    "  text = re.sub(r'[^\\w\\s]', '', text) # 특수문자 제거\n",
    "  return text\n",
    "\n",
    "normalized_reviews = [normalize_text(' '.join(review)) for review, category in documents]\n",
    "\n",
    "# 2. 어간추출 (stemming)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_reviews = [' '.join([stemmer.stem(word) for word in review.split()]) for review in normalized_reviews]\n",
    "\n",
    "# 3. 표제어추출\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_reviews = [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in stemmed_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tVioPuKWnkk",
    "outputId": "ec8bd44b-024e-4fb1-b9b4-277ce70b8610"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma & CountVectorizer': 0.82,\n",
       " 'lemma & min_df': 0.82,\n",
       " 'lemma & bigram': 0.8625,\n",
       " 'lemma & tfidf': 0.8}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 4. CounterVectorizer 하이퍼파라미터 조정\n",
    "# 최소 빈도수 설정해서 희귀 단어 제거 가능\n",
    "\n",
    "# 2번의 어간추출, 표제어추출로 학습 테스트\n",
    "vectorizer_min_df = CountVectorizer(min_df = 2) # min_df=2 → 최소 2개 이상의 문서에서 등장하는 단어만 사용 (희귀 단어 제거)\n",
    "X_min_df = vectorizer_min_df.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# N-gram 사용 (2,2) \n",
    "vectorizer_bigram = CountVectorizer(ngram_range = (2,2)) # ngram_range=(2,2) → 2개의 단어씩 묶어서 벡터화\n",
    "X_bigram = vectorizer_bigram.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# TF-IDF -> 자주 등장하는 일반적인 단어의 중요도를 낮추고, 특정 문서에서만 많이 등장하는 단어의 가중치를 높임\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(lemmatized_reviews)\n",
    "\n",
    "## 데이터 분할 (어떤 벡터화 방법이 가장 적절한지 확인해보기)\n",
    "\n",
    "# 1. stem\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split(lemmatized_reviews, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 2. min_df \n",
    "X_train_min_df, X_test_min_df, y_train_min_df, y_test_min_df = train_test_split(X_min_df, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 3. bigram\n",
    "X_train_bigram, X_test_bigram, y_train_bigram, y_test_bigram = train_test_split(X_bigram, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "# 4. TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, [label for _, label in documents], test_size = 0.2, random_state = 111)\n",
    "\n",
    "## Naive Bayes 분류기 사용하여 학습, 평가\n",
    "# 텍스트 분류에서 자주 사용되는 베이즈 모델 -> 단어 등장 확률을 기반으로 문서가 특정 클래스에 속할 확률을 계산\n",
    "# 1. stem\n",
    "model_stem = MultinomialNB()\n",
    "model_stem.fit(vectorizer.fit_transform(X_train_stem), y_train_stem)\n",
    "y_pred_stem = model_stem.predict(vectorizer.transform(X_test_stem))\n",
    "accuracy_stem = accuracy_score(y_test_stem, y_pred_stem)\n",
    "\n",
    "# 2. min_df\n",
    "model_min_df = MultinomialNB()\n",
    "model_min_df.fit(X_train_min_df, y_train_min_df)\n",
    "y_pred_min_df = model_min_df.predict(X_test_min_df)\n",
    "accuracy_min_df = accuracy_score(y_test_min_df, y_pred_min_df)\n",
    "\n",
    "# 3. bigram\n",
    "model_bigram = MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, y_train_bigram)\n",
    "y_pred_bigram = model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(y_test_bigram, y_pred_bigram)\n",
    "\n",
    "# 4. TF-IDF\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n",
    "\n",
    "# 각 벡터화 방법별 정확도(accuracy)를 비교\n",
    "# 가장 높은 정확도를 보이는 방법을 선택하여 최종 모델을 결정 가능\n",
    "\n",
    "{\n",
    "    'lemma & CountVectorizer': accuracy_stem,\n",
    "    'lemma & min_df': accuracy_min_df,\n",
    "    'lemma & bigram': accuracy_bigram,\n",
    "    'lemma & tfidf': accuracy_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPZnDd4KKH4A"
   },
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGvTWjE5KHC8"
   },
   "outputs": [],
   "source": [
    "# 정규표현식을 통한 토크나이저 재정의\n",
    "# 그리드 서치를 통한 하이퍼파라미터 조정 (alpha)\n",
    "\n",
    "# 이 외에도 다양한 시도를 통해 최종적으로 성능을 높이는 것이 이번 필수과제의 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGRHgVBxaeyY",
    "outputId": "7b994b60-2169-4f96-f992-6e8befad4a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터: {'alpha': 1.0}\n",
      "최고의 교차 검증 점수: 0.83125\n"
     ]
    }
   ],
   "source": [
    "# 그리드서치\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]}\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv = 5, scoring = 'accuracy')\n",
    "grid_search.fit(X_train_bigram, y_train_bigram)\n",
    "\n",
    "# 최적의 하이퍼파라미터와 성능 출력\n",
    "print(f\"최적의 파라미터: {grid_search.best_params_}\")\n",
    "print(f\"최고의 교차 검증 점수: {grid_search.best_score_}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
